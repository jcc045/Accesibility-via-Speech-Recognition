import tkinter as tk
from tkinter import colorchooser
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import scipy.spatial.distance as distance
import torchaudio
import torchaudio.transforms
from sklearn.metrics.pairwise import cosine_similarity
#from sklearn import torchaudio 
import torch
import torch.nn.functional as F



class BiometricSystem:
    button_style = {'bg': '#3498db', 'fg': 'white', 'borderwidth': 2, 'relief': tk.GROOVE, 'font': ('Helvetica', 12)}
    print(str(torchaudio.get_audio_backend()))
    def __init__(self, master):
        self.master = master
        self.master.title("Biometric System")
        self.master.geometry("400x400")
        self.master.resizable(False, False)

        self.current_frame = None
        self.name_value = ""
        self.color = ""
        self.audio_data = None

        self.create_main_frame()

    def create_main_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        enroll_button = tk.Button(self.current_frame, text="Enroll", command=self.switch_to_name_frame, **self.button_style)
        enroll_button.pack(pady=20)

        authenticate_button = tk.Button(self.current_frame, text="Authenticate", command=self.authenticate_action, **self.button_style)
        authenticate_button.pack(pady=20)

    def switch_to_name_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        name_label = tk.Label(self.current_frame, text="Enter Your Name:")
        name_label.pack(pady=5)

        self.name_entry = tk.Entry(self.current_frame, width=30)
        self.name_entry.pack(pady=5)

        next_button = tk.Button(self.current_frame, text="Next", command=self.switch_to_color_frame, **self.button_style)
        next_button.pack(pady=20)

    def switch_to_color_frame(self):
        self.name_value = self.name_entry.get()  # Store the name value
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        color_label = tk.Label(self.current_frame, text="Choose a Color:")
        color_label.pack(pady=5)

        color_button = tk.Button(self.current_frame, text="Choose Color", command=self.choose_color)
        color_button.pack(pady=5)

        next_button = tk.Button(self.current_frame, text="Next", command=self.switch_to_audio_frame, **self.button_style)
        next_button.pack(pady=20)

    def switch_to_audio_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        prompt_label = tk.Label(self.current_frame, text=f"Hello, {self.name_value}! Please say the following phrase: Two blue fish swam in the tank.")
        prompt_label.pack(pady=10)

        record_button = tk.Button(self.current_frame, text="Record Audio", command=self.record_audio)
        record_button.pack(pady=10)

        stop_button = tk.Button(self.current_frame, text="Stop Recording", command=self.stop_recording, **self.button_style)
        stop_button.pack(pady=10)

    def choose_color(self):
        _, self.color = colorchooser.askcolor(title="Choose a color")

    def record_audio(self):
        print(f"Recording...")
        self.audio_data = sd.rec(44100 * 5, samplerate=44100, channels=2, dtype=np.int16)

    def stop_recording(self):
        sd.stop()
        filename = f"{self.name_value}_enrollment_recording.wav"
        wav.write(filename, 44100, self.audio_data)
        print(f"Audio recorded and saved to {filename}")

        with open('enrollment_info.txt', 'a') as file:
            file.write(f"Name: {self.name_value}\nColor: {self.color}\nAudio File: {filename}\n\n")

        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame
        self.create_main_frame()  # Show the main frame again

    def authenticate_action(self):
        if self.current_frame:
            self.current_frame.pack_forget()

        authentication_frame = tk.Frame(self.master)
        authentication_frame.pack()

        prompt_label = tk.Label(authentication_frame, text="Please say the authentication phrase: Two blue fish swam in the tank.")
        prompt_label.pack(pady=10)

        record_button = tk.Button(authentication_frame, text="Record Audio", command=self.record_audio_authenticate)
        record_button.pack(pady=10)

        stop_button = tk.Button(authentication_frame, text="Stop Recording", command=self.stop_recording_authenticate, **self.button_style)
        stop_button.pack(pady=10)

    def record_audio_authenticate(self):
        print(f"Recording...")
        self.audio_data = sd.rec(44100 * 5, samplerate=44100, channels=2, dtype=np.int16)

    def stop_recording_authenticate(self):
        sd.stop()

        # Compare the recorded audio with saved recordings
        match_threshold = 0.85
        matched_user = self.match_recordings(self.audio_data, match_threshold)

        if matched_user:
            print(f"Authentication successful! Welcome, {matched_user}.")
            # Display the authenticated message
            self.display_authenticated_user(matched_user)
        else:
            print("Authentication failed. Please try again.")

        # Hide the current frame after authentication
        if self.current_frame:
            self.current_frame.pack_forget()

        # Show the main frame again only if it's not already created
        if not hasattr(self, 'main_frame_created') or not self.main_frame_created:
            self.create_main_frame()
            self.main_frame_created = True

    def match_recordings(self, audio_data, threshold):
        # Load and compare the audio data with saved recordings
        max_similarity = -1
        matched_user = None

        with open('enrollment_info.txt', 'r') as file:
            lines = file.readlines()
            for i in range(0, len(lines), 4):  # Adjust the step to 4
                user_name = lines[i + 1].split(":")[1].strip()
                saved_audio_data = self.load_audio_data(lines[i + 2].split(":")[1].strip())  # Use lines[i + 2] for audio file

                if saved_audio_data is not None:

                    # Print the current user being checked
                    print(f"Checking user: {user_name}")

                    # Compare the audio data with the new recording
                    similarity = self.compare_audio(audio_data.flatten(), saved_audio_data.flatten())

                    if similarity >= threshold and similarity > max_similarity:
                        max_similarity = similarity
                        matched_user = lines[i].split(":")[1].strip()

        return matched_user

    #def load_audio_data(self, filename):
    #    try:
    #        _, audio_data = wav.read(filename)
    #        return audio_data
    #    except FileNotFoundError:
    #        return None
    
    #def load_audio_data(self, filename):
    #    try:
    #        audio_data, _ = torchaudio.load(filename)
    #        return audio_data.numpy()
    #    except FileNotFoundError:
    #        return None
        
    def load_audio_data(self, filename):
        try:
            audio_data, _ = torchaudio.load(filename, normalize=True, backend="soundfile")
            return audio_data.numpy()
        except FileNotFoundError:
            return None

    #def compare_audio(self, audio_data1, audio_data2):
        # Apply necessary transformations or feature extraction
        # For example, you might want to use MFCC features
        #mfcc_transform = torchaudio.transforms.MFCC()

        # Apply the transformation to the audio data
        #features1 = mfcc_transform(audio_data1)
        #features2 = mfcc_transform(audio_data2)

        # Reshape the features to 2D
        #features1 = features1.view(features1.size(0), -1)
        #features2 = features2.view(features2.size(0), -1)

        # Calculate cosine similarity
        #similarity_matrix = cosine_similarity(features1.numpy(), features2.numpy())

        # Average the similarity scores
        #similarity = similarity_matrix.mean()

        #return similarity

    def compare_audio(self, audio_data1, audio_data2):
        # Use torchaudio.transforms.MFCC directly
        mfcc_transform = torchaudio.transforms.MFCC()

        # Apply MFCC transform to audio data
        features1 = mfcc_transform(torch.tensor(audio_data1).float())
        features2 = mfcc_transform(torch.tensor(audio_data2).float())

        # Compare the transformed features
        return 1 - F.cosine_similarity(features1, features2)

    def display_authenticated_user(self, user):
        authenticated_label = tk.Label(self.master, text=f"Authentication Successful! Welcome, {user}.", font=('Helvetica', 16))
        authenticated_label.pack(pady=20)
        self.master.after(5000, lambda: authenticated_label.pack_forget())  # Display for 5 seconds and then hide


if __name__ == "__main__":
    root = tk.Tk()
    app = BiometricSystem(root)
    root.mainloop()
