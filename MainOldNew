import tkinter as tk
from tkinter import colorchooser
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import scipy.spatial.distance as distance
import torchaudio
import torchaudio.transforms as T
from sklearn.metrics.pairwise import cosine_similarity
import torch
import torch.nn.functional as F



class BiometricSystem:
    button_style = {'bg': '#3498db', 'fg': 'white', 'borderwidth': 2, 'relief': tk.GROOVE, 'font': ('Helvetica', 12)}
    #print(str(torchaudio.get_audio_backend()))
    
    def __init__(self, master):
        self.master = master
        self.master.title("Biometric System")
        self.master.geometry("400x400")
        self.master.resizable(False, False)

        self.current_frame = None
        self.name_value = ""
        self.color = ""
        self.audio_data = None

        self.create_main_frame()

    def create_main_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        enroll_button = tk.Button(self.current_frame, text="Enroll", command=self.switch_to_name_frame, **self.button_style)
        enroll_button.pack(pady=20)

        authenticate_button = tk.Button(self.current_frame, text="Authenticate", command=self.authenticate_action, **self.button_style)
        authenticate_button.pack(pady=20)

    def switch_to_name_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        name_label = tk.Label(self.current_frame, text="Enter Your Name:")
        name_label.pack(pady=5)

        self.name_entry = tk.Entry(self.current_frame, width=30)
        self.name_entry.pack(pady=5)

        next_button = tk.Button(self.current_frame, text="Next", command=self.switch_to_color_frame, **self.button_style)
        next_button.pack(pady=20)

    def switch_to_color_frame(self):
        self.name_value = self.name_entry.get()  # Store the name value
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        color_label = tk.Label(self.current_frame, text="Choose a Color:")
        color_label.pack(pady=5)

        color_button = tk.Button(self.current_frame, text="Choose Color", command=self.choose_color)
        color_button.pack(pady=5)

        next_button = tk.Button(self.current_frame, text="Next", command=self.switch_to_audio_frame, **self.button_style)
        next_button.pack(pady=20)

    def switch_to_audio_frame(self):
        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame

        self.current_frame = tk.Frame(self.master)
        self.current_frame.pack()

        prompt_label = tk.Label(self.current_frame, text=f"Hello, {self.name_value}! Please say the following phrase: Two blue fish swam in the tank.")
        prompt_label.pack(pady=10)

        record_button = tk.Button(self.current_frame, text="Record Audio", command=self.record_audio)
        record_button.pack(pady=10)

        stop_button = tk.Button(self.current_frame, text="Stop Recording", command=self.stop_recording, **self.button_style)
        stop_button.pack(pady=10)

    def choose_color(self):
        _, self.color = colorchooser.askcolor(title="Choose a color")

    def record_audio(self):
        print(f"Recording...")
        self.audio_data = sd.rec(44100 * 5, samplerate=44100, channels=2, dtype=np.int16)
        sd.wait()

        # Trim silence using a threshold (adjust threshold as needed)
        threshold = 0.01
        trimmed_audio_data = self.trim_silence(self.audio_data, threshold)

        self.audio_data = trimmed_audio_data

    def stop_recording(self):
        sd.stop()
        filename = f"{self.name_value}_enrollment_recording.wav"
        wav.write(filename, 44100, self.audio_data)
        print(f"Audio recorded and saved to {filename}")

        with open('enrollment_info.txt', 'a') as file:
            file.write(f"Name: {self.name_value}\nColor: {self.color}\nAudio File: {filename}\n\n")

        if self.current_frame:
            self.current_frame.pack_forget()  # Hide the current frame
        self.create_main_frame()  # Show the main frame again

    def authenticate_action(self):
        if self.current_frame:
            self.current_frame.pack_forget()

        authentication_frame = tk.Frame(self.master)
        authentication_frame.pack()

        prompt_label = tk.Label(authentication_frame, text="Please say the authentication phrase: Two blue fish swam in the tank.")
        prompt_label.pack(pady=10)

        record_button = tk.Button(authentication_frame, text="Record Audio", command=self.record_audio_authenticate)
        record_button.pack(pady=10)

        stop_button = tk.Button(authentication_frame, text="Stop Recording", command=self.stop_recording_authenticate, **self.button_style)
        stop_button.pack(pady=10)

    def record_audio_authenticate(self):
        print(f"Recording...")
        self.audio_data = sd.rec(44100 * 5, samplerate=44100, channels=2, dtype=np.int16)
        sd.wait()

        # Trim silence using a threshold (adjust threshold as needed)
        threshold = 0.01
        trimmed_audio_data = self.trim_silence(audio_data, threshold)

        self.audio_data = trimmed_audio_data

    def stop_recording_authenticate(self):
        sd.stop()

        # Compare the recorded audio with saved recordings
        match_threshold = 0.85
        matched_user = self.match_recordings(self.audio_data, match_threshold)

        if matched_user:
            print(f"Authentication successful! Welcome, {matched_user}.")
            # Display the authenticated message
            self.display_authenticated_user(matched_user)
        else:
            print("Authentication failed. Please try again.")

        # Hide the current frame after authentication
        if self.current_frame:
            self.current_frame.pack_forget()

        # Show the main frame again only if it's not already created
        if not hasattr(self, 'main_frame_created') or not self.main_frame_created:
            self.create_main_frame()
            self.main_frame_created = True

    def match_recordings(self, audio_data, threshold):
        # Load and compare the audio data with saved recordings
        max_similarity = -1
        matched_user = None

        with open('enrollment_info.txt', 'r') as file:
            lines = file.readlines()
            for line in lines:
                saved_audio_data = self.load_audio_data(line.strip())  # Use the entire line as the audio file name

                if saved_audio_data is not None:
                    # Compare the audio data with the new recording
                    similarity = self.compare_audio(audio_data, saved_audio_data)

                    if similarity >= threshold and similarity > max_similarity:
                        max_similarity = similarity
                        matched_user = line.split("_")[0].strip()  # Extract the user name from the file name

        return matched_user
    
    def trim_silence(self, audio_data, threshold):
        # Find indices where audio amplitude is above the threshold
        indices = np.where(np.max(np.abs(audio_data), axis=1) > threshold)[0]

        # Trim silence
        trimmed_audio_data = audio_data[indices[0]:indices[-1]+1, :]

        return trimmed_audio_data
        
    def load_audio_data(self, filename):
        try:
            audio_data, _ = torchaudio.load(filename, normalize=True, backend="soundfile")
            return audio_data.numpy()
        except FileNotFoundError:
            return None

    #def compare_audio(self, audio_data1, audio_data2):
        # Apply necessary transformations or feature extraction
        # For example, you might want to use MFCC features
        #mfcc_transform = torchaudio.transforms.MFCC()

        # Apply the transformation to the audio data
        #features1 = mfcc_transform(audio_data1)
        #features2 = mfcc_transform(audio_data2)

        # Reshape the features to 2D
        #features1 = features1.view(features1.size(0), -1)
        #features2 = features2.view(features2.size(0), -1)

        # Calculate cosine similarity
        #similarity_matrix = cosine_similarity(features1.numpy(), features2.numpy())

        # Average the similarity scores
        #similarity = similarity_matrix.mean()

        #return similarity

    def compare_audio(self, audio_data1, audio_data2):
        transform = T.MFCC()
        
        # Convert audio data to float32 if it's in integer format
        if audio_data1.dtype.kind == 'i':
            audio_data1 = audio_data1.astype(np.float32) / np.iinfo(audio_data1.dtype).max

        if audio_data2.dtype.kind == 'i':
            audio_data2 = audio_data2.astype(np.float32) / np.iinfo(audio_data2.dtype).max

        # Apply transformations to obtain features
        features1 = transform(torch.tensor(audio_data1).float())
        features2 = transform(torch.tensor(audio_data2).float())

        # Ensure both features have the same number of dimensions
        max_channels = max(features1.shape[0], features2.shape[0])
        features1 = F.pad(features1, (0, 0, 0, max_channels - features1.shape[0]))
        features2 = F.pad(features2, (0, 0, 0, max_channels - features2.shape[0]))

        # Determine the maximum length of the two features
        max_length = max(features1.shape[2], features2.shape[2])

        # Pad features to the maximum length
        features1 = F.pad(features1, (0, max_length - features1.shape[2]))
        features2 = F.pad(features2, (0, max_length - features2.shape[2]))

        # Calculate similarity (cosine distance)
        similarity = 1 - F.cosine_similarity(features1, features2, dim=2)

        return similarity.item()

    
    def calculate_similarity(self, features1, features2):
        # You can use a suitable similarity metric here
        # For example, you can use cosine similarity:
        similarity = torch.nn.functional.cosine_similarity(features1, features2, dim=0).item()
        
        return similarity

    def display_authenticated_user(self, user):
        authenticated_label = tk.Label(self.master, text=f"Authentication Successful! Welcome, {user}.", font=('Helvetica', 16))
        authenticated_label.pack(pady=20)
        self.master.after(5000, lambda: authenticated_label.pack_forget())  # Display for 5 seconds and then hide


if __name__ == "__main__":
    root = tk.Tk()
    app = BiometricSystem(root)
    root.mainloop()
